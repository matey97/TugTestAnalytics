{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "valuable-bacteria",
   "metadata": {},
   "source": [
    "# TUG Test Preprocessing and Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "favorite-southeast",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "square-request",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "from math import sqrt\n",
    "from functools import reduce\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from segmentation import SEGMENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "given-eugene",
   "metadata": {},
   "source": [
    "## Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "preliminary-potential",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"DATA\"\n",
    "FEATURES_DIR = \"FEATURES\"\n",
    "\n",
    "## MAX RESOLUTION OF DEVICE SENSOR.\n",
    "ACC_MAX_RES = 78.4532\n",
    "GYRO_MAX_RES = 34.906586"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "julian-gardening",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surface-locking",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "muslim-there",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_file_desc(data_file):\n",
    "    file_path = os.path.join(DATA_DIR, data_file)\n",
    "    file_desc = data_file.split(\"_\")\n",
    "    file_desc = \"_\".join(file_desc[0:2])\n",
    "    \n",
    "    return file_path, file_desc\n",
    "\n",
    "\n",
    "def load_data_file(data_file_path):\n",
    "    with open(data_file_path) as file:\n",
    "        return json.load(file) \n",
    "    \n",
    "    \n",
    "def to_dataframe(records_file):\n",
    "    tidy_records = []\n",
    "\n",
    "    for records in records_file:\n",
    "        for record in records['records']:\n",
    "            tidy_records.append({\n",
    "                \"type\": records['type'],\n",
    "                \"timestamp\": record['timestamp'],\n",
    "                \"x\": record['x'],\n",
    "                \"y\": record['y'],\n",
    "                \"z\": record['z']\n",
    "            })\n",
    "    df = pd.DataFrame(tidy_records)\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animal-doubt",
   "metadata": {},
   "source": [
    "###  Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "broken-current",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_components(dataframe, components):\n",
    "    sensor_types = dataframe.type.unique()\n",
    "    n_subplots = len(sensor_types)\n",
    "    fig = make_subplots(rows=n_subplots, cols=1, subplot_titles=sensor_types, shared_xaxes=True)\n",
    "    \n",
    "    for i, sensor_type in enumerate(dataframe.type.unique()):\n",
    "        sensor_df = dataframe[dataframe.type == sensor_type]\n",
    "        for component in components:\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=sensor_df.timestamp, y=sensor_df[component], name=\"{0} {1}\".format(sensor_type, component)),\n",
    "                row=i+1, col=1,\n",
    "            )\n",
    "        \n",
    "    fig.update_layout(height=400*n_subplots)\n",
    "    fig.update_yaxes(title_text=\"Magnitude\")\n",
    "    fig.show()\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "def plot_triaxial_components(dataframe, components_group=[['x_acc', 'y_acc', 'z_acc'], ['x_gyro', 'y_gyro', 'z_gyro']]):\n",
    "    n_subplots = len(components_group)\n",
    "    fig = make_subplots(rows=n_subplots, cols=1, shared_xaxes=True)\n",
    "    \n",
    "    for i, group in enumerate(components_group):\n",
    "        for component in group:\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=dataframe.timestamp_acc, y=dataframe[component], name=component),\n",
    "                row=i+1, col=1,\n",
    "            )\n",
    "            \n",
    "    fig.update_layout(height=400*n_subplots)\n",
    "    fig.update_yaxes(title_text=\"Magnitude\")\n",
    "    fig.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rough-maintenance",
   "metadata": {},
   "source": [
    "### Data Cleaning (Temporal alignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "victorian-angola",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(dataframe, types=['ACCELEROMETER', 'GYROSCOPE']):\n",
    "    splits = []\n",
    "    for typ in types:\n",
    "        splits.append(dataframe[dataframe.type == typ])\n",
    "    return splits\n",
    "\n",
    "\n",
    "def temporal_trim(df1, df2):\n",
    "    def global_start():\n",
    "        start_df1 = df1['timestamp'].min()\n",
    "        start_df2 = df2['timestamp'].min()\n",
    "        return max(start_df1, start_df2)\n",
    "    \n",
    "    def global_end():\n",
    "        end_df1 = df1['timestamp'].max()\n",
    "        end_df2 = df2['timestamp'].max()\n",
    "        return min(end_df1, end_df2)\n",
    "    \n",
    "    def trim(df, start, end):\n",
    "        start_thresh = start.replace(microsecond=start.microsecond - 1000)\n",
    "        end_thresh = end.replace(microsecond=end.microsecond + 1000)\n",
    "        return df[(df.timestamp >= start_thresh) & (df.timestamp <= end_thresh)]\n",
    "    \n",
    "    start = global_start()\n",
    "    end = global_end()\n",
    "    \n",
    "    return trim(df1, start, end), trim(df2, start, end)\n",
    "    \n",
    "\n",
    "def merge(df_acc, df_gyro):\n",
    "    def copy_reset(df):\n",
    "        return df.copy().reset_index(drop=True)\n",
    "    \n",
    "    def drop(df, columns):\n",
    "        return df.drop(columns, axis=1)\n",
    "    \n",
    "    df_acc = drop(copy_reset(df_acc), ['type'])\n",
    "    df_gyro = drop(copy_reset(df_gyro), ['type'])\n",
    "    \n",
    "    df_merged = df_acc.join(df_gyro, lsuffix=\"_acc\", rsuffix=\"_gyro\")\n",
    "    return df_merged.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bound-virgin",
   "metadata": {},
   "source": [
    "### Data Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "noted-detective",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_labels(desc, df):\n",
    "    df_copy = df.copy()\n",
    "    segments = SEGMENTS[desc]\n",
    "    def compute_label(timestamp):\n",
    "        time = timestamp.time()\n",
    "        if time < segments['stand_start'] or time >= segments['sit_end']:\n",
    "            return 'SIT'\n",
    "        if segments['stand_start'] <= time < segments['stand_end']:\n",
    "            return 'STANDING'\n",
    "        if segments['turn1_start'] <= time < segments['turn1_end'] or segments['turn2_start'] <= time < segments['turn2_end']:\n",
    "            return 'TURNING'\n",
    "        if segments['sit_start'] <= time < segments['sit_end']:\n",
    "            return 'SITTING'\n",
    "        return 'WALKING' \n",
    "    \n",
    "    df_copy['gt'] = df_copy.timestamp_acc.apply(compute_label)\n",
    "    \n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sized-condition",
   "metadata": {},
   "source": [
    "### Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "shared-layout",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(value, maxVal, minVal, ran=[-1, 1]):\n",
    "    return (ran[1] - ran[0]) * (value - minVal) / (maxVal - minVal) + ran[0]\n",
    "\n",
    "\n",
    "def normalize_acc(value):\n",
    "    return norm(value, ACC_MAX_RES, -ACC_MAX_RES)\n",
    "    \n",
    "    \n",
    "def normalize_gyro(value):\n",
    "    return norm(value, GYRO_MAX_RES, -GYRO_MAX_RES)\n",
    "\n",
    "\n",
    "def normalize(df):\n",
    "    df_copy = df.copy()\n",
    "    df_copy[['x_acc', 'y_acc', 'z_acc']] = df_copy[['x_acc', 'y_acc', 'z_acc']].apply(normalize_acc)\n",
    "    df_copy[['x_gyro', 'y_gyro', 'z_gyro']] = df_copy[['x_gyro', 'y_gyro', 'z_gyro']].apply(normalize_gyro)\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embedded-property",
   "metadata": {},
   "source": [
    "### Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prospective-confirmation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def windows(data, window_size, step):\n",
    "    r = np.arange(len(data))\n",
    "    s = r[::step]\n",
    "    z = list(zip(s, s + window_size))\n",
    "    f = '{0[0]}:{0[1]}'.format\n",
    "    g = lambda step : data.iloc[step[0]:step[1]]\n",
    "    return pd.concat(map(g, z), keys=map(f, z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continued-chorus",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(wdf, original_shape):\n",
    "    df = pd.DataFrame()\n",
    "    for i in range(0, original_shape, STEP_SIZE):\n",
    "        window = wdf.loc[\"{0}:{1}\".format(i, i+WINDOW_SIZE)]\n",
    "        #normalized_values= window[['x_acc_norm', 'y_acc_norm', 'z_acc_norm', 'x_gyro_norm', 'y_gyro_norm', 'z_gyro_norm']]\n",
    "        normalized_values= window[['x_acc', 'y_acc', 'z_acc', 'x_gyro', 'y_gyro', 'z_gyro']]\n",
    "        \n",
    "        mean_s = normalized_values.mean(axis=0)\n",
    "        median_s = normalized_values.median(axis=0)\n",
    "        max_s = normalized_values.max(axis=0)\n",
    "        min_s = normalized_values.min(axis=0)\n",
    "        std_s = normalized_values.std(axis=0)\n",
    "        range_s = max_s - min_s\n",
    "        rms_s = rms_series(window)\n",
    "        pitch_roll_s = pitch_and_roll(window)\n",
    "        angle_s = compute_gyro_angle(window)\n",
    "        \n",
    "        ground_truth = compute_best_class(window)\n",
    "        \n",
    "        window_features = join_series([mean_s, median_s, max_s, min_s, std_s, range_s, rms_s, pitch_roll_s, angle_s, ground_truth],\n",
    "                                      [\"_mean\", \"_median\", \"_max\", \"_min\", \"_std\", \"_range\", \"\", \"\", \"\", \"\"])\n",
    "        \n",
    "        df = pd.concat([df, window_features], ignore_index=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def join_series(series, suffixes):\n",
    "    for i, serie in enumerate(series):\n",
    "        current_df = serie.to_frame().transpose()\n",
    "        if i == 0:\n",
    "            df = current_df\n",
    "        else:\n",
    "            df = df.join(current_df, lsuffix=suffixes[i-1], rsuffix=suffixes[i])\n",
    "    return df\n",
    "\n",
    "\n",
    "def to_series(data, index):\n",
    "    return pd.Series(data, index=index)\n",
    "       \n",
    "    \n",
    "def rms_series(df):\n",
    "    def rms(values):\n",
    "        return sqrt(reduce(lambda prev, curr: prev + curr ** 2, values, 0) / len(values))\n",
    "    \n",
    "    return to_series([\n",
    "        rms(df.x_acc), rms(df.y_acc), rms(df.z_acc),\n",
    "        rms(df.x_gyro), rms(df.y_gyro), rms(df.z_gyro)\n",
    "    ], ['x_acc_rms','y_acc_rms','z_acc_rms', 'x_gyro_rms','y_gyro_rms','z_gyro_rms'])\n",
    "    \n",
    "    \n",
    "def pitch_and_roll(df):\n",
    "    def angular_function(a, b):\n",
    "        return np.arctan2(a, b) * 180/np.pi\n",
    "    \n",
    "    return to_series([\n",
    "        angular_function(df.y_acc, df.z_acc).mean(),\n",
    "        angular_function(-df.x_acc, np.sqrt(np.power(df.y_acc, 2) + np.power(df.z_acc, 2))).mean()\n",
    "    ], ['pitch', 'roll'])\n",
    " \n",
    "    \n",
    "def integrator(series):\n",
    "    return np.trapz(series)\n",
    "\n",
    "\n",
    "def compute_gyro_angle(df):\n",
    "    return to_series([integrator(df.x_gyro), integrator(df.y_gyro), integrator(df.z_gyro)], ['x_angle', 'y_angle', 'z_angle'])\n",
    "\n",
    "\n",
    "def compute_best_class(df):\n",
    "    ground_truth = df['gt'].value_counts()\n",
    "    #if len(ground_truth) == 1 or ground_truth.max() > ground_truth.sum() * 3 / 4: #Only one class or 75% of samples are from majority class\n",
    "    best = ground_truth.index[0]\n",
    "    #else:\n",
    "    #    best = 'MIXED' \n",
    "        \n",
    "    return to_series([best], ['CLASS'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noble-socket",
   "metadata": {},
   "source": [
    "## Play"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sitting-therapist",
   "metadata": {},
   "source": [
    "### Load raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "generic-owner",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = {}\n",
    "\n",
    "for data_file in os.listdir(DATA_DIR):\n",
    "    file_path, file_desc = data_file_desc(data_file)\n",
    "    if not os.path.isfile(file_path):\n",
    "        continue\n",
    "    records_file = load_data_file(file_path)\n",
    "    raw_data[file_desc] = to_dataframe(records_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confused-chicago",
   "metadata": {},
   "source": [
    "### Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "searching-comment",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clean_data = {}\n",
    "\n",
    "for desc, data in raw_data.items():\n",
    "    df_acc, df_gyro = split(data)\n",
    "    df_acc, df_gyro = temporal_trim(df_acc, df_gyro)    \n",
    "    df_merged = merge(df_acc, df_gyro)\n",
    "    \n",
    "    clean_data[desc] = normalize(add_labels(desc, df_merged))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "falling-henry",
   "metadata": {},
   "source": [
    "### Extract and save features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "friendly-headline",
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 50\n",
    "STEP_SIZE = WINDOW_SIZE // 2\n",
    "\n",
    "features_df = pd.DataFrame()\n",
    "for desc, data in clean_data.items():\n",
    "    windowed_df = windows(data, WINDOW_SIZE, STEP_SIZE)\n",
    "    window_features = extract_features(windowed_df, data.shape[0])\n",
    "    features_df = pd.concat([features_df, window_features], ignore_index=True)\n",
    "    \n",
    "features_df.to_csv(os.path.join(FEATURES_DIR, 'features_full_norm_01.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
